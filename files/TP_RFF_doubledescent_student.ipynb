{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_s_curve\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the classical bias variance tradeoff: \n",
    "- A model with too low complexity might not be able to interpolate the training data. This is **underfitting**, there is too much **biais**. \n",
    "- A model with too high complexity might interpolate the training data with no generalization to the test data. This is **overfitting**, there is too much **variance**.  \n",
    "\n",
    "This typically lead to the following learning curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Bias_and_variance_contributing_to_total_error.svg/1920px-Bias_and_variance_contributing_to_total_error.svg.png\" alt=\"Alt text\" width=\"700\"  style=\"background-color: white\" > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The double descent is an intersesting phenomemon where beyond a certain point of model complexity, test performance may improve again, resulting in the so-called \"double descent\" pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/50/Double_descent.png\" alt=\"Alt text\" width=\"700\"  style=\"background-color: white\" > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The double descent phenomenom has recently gained interest in the Machine Learning community as it contributes to lighten the behaviour of Deep neural Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that double descent occurs on a very specific setting. Denoting $p$ the number of parameters in the model (i.e. its complexity) and $n$ the number of points in the dataset we can defien the **relative complexity** of the model as\n",
    "\n",
    "$$ C = \\frac{p}{n}$$\n",
    "\n",
    "Double descent typically occurs when $p \\to \\infty$ (very large model), $n \\to \\infty$ (very large dataset) but $C$ is fixed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setting is very common in deep learning but it has also been extensively studied in the literature at the lens of Kernel Ridgeless Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we will approximate Kernel Ridgeless Regression with $p$ Random Fourier Features and observe double descent occur as $p \\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Fourier Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for any kernel $k(x,y)$ there exist a feature map $\\phi : \\mathbf{R}^d \\mapsto \\mathcal{H}$ such that \n",
    "\n",
    "$$ k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle$$\n",
    "\n",
    "Usually, we never compute $\\phi(x)$ has it can be very complicated (typically of infinite dimension). Instead we prefer to use the representer theorem which tells us that most problem can be solved by computing the Gram matrix $K$ of the train set:\n",
    "\n",
    "$$ K_{i,j} = k(x_i,x_j) $$\n",
    "\n",
    "The problem is that this matrix has $n^2$ entries which makes it unpractical (or even impossible) to compute/store for large dataset (as considered in this practical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it mean that kernel methods do not apply to large datasets ? No ! This is where Random Fourier Features (or others kernel approximations techniques) comes into play.\n",
    "\n",
    "The goal is now to find a feature map $\\psi : \\mathbf{R}^d \\mapsto \\mathbf{R}^p$ that **can** be computed (unlike to $\\phi$) such that \n",
    "\n",
    "$$ k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle \\approx \\langle \\psi(x), \\psi(y) \\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a kernel of the following form:\n",
    "\n",
    "$$ k(x,y) = h(x - y)$$\n",
    "\n",
    "For instance this is the case of the Gaussian kernel for $h(z) = \\exp(z^2/\\sigma^2)$.\n",
    "\n",
    "It is a standard results from Fourier transform theory that (for well behaved functions $h$)\n",
    "\n",
    "$$ h(z) = \\int_w p(w) \\exp( i \\langle z, w \\rangle ) dw$$\n",
    "\n",
    "if $p(w)$ is set to be the Fourier transform of $h$. Assuming that $p$ is normalized to be a probability distribution we get that \n",
    "\n",
    "$$ k(x,y) = E_w[ \\exp(i \\langle x - y, w \\rangle) ]$$\n",
    "\n",
    "For $ w \\sim p(w)$. Since k is real we have even remove the the imaginary part to get that \n",
    "\n",
    "$$ k(x,y) = E_w[ \\cos(\\langle x - y, w \\rangle) ]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Lets $b \\sim \\text{Unif}(0,2\\pi)$ prove that \n",
    "\n",
    "$$  E_w[ \\cos(\\langle x - y, w \\rangle) ] = 2 E_{w,b}[ \\cos(\\langle x, w  \\rangle+ b) \\cos(\\langle y, w \\rangle+ b ) ]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ANSWER HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the standard Monte Carle approximation to the previous results we get that:\n",
    "\n",
    "$$ k(x,y) \\approx \\frac{2}{p} \\sum_{i=1}^p \\cos(\\langle x, w_i  \\rangle+ b_i) \\cos(\\langle y, w_i \\rangle+ b_i )$$\n",
    "\n",
    "For some $w_i \\sim p(w)$ and $b_i \\sim \\text{Unif}(0,2\\pi)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Define the feature map $\\psi : \\mathbf{R}^d \\mapsto \\mathbf{R}^p$ such that \n",
    "\n",
    "$$ k(x,y) \\approx \\langle \\psi(x), \\psi(y) \\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ANSWER HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) Now lets write the code to compute an approximation of the Gram Matrix $K$ for the Gaussian kernel\n",
    "\n",
    "$$ k(x,y) = \\exp\\left( - \\frac{1}{2} ||x-y||^2 \\right)$$\n",
    "\n",
    "Note that for the Gaussian Kernel you should sample $w_i$ from $N(0,1)$ as the Fourier transform of a Gaussian is also a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_Exact_Gaussian_Kernel(X):\n",
    "    \n",
    "    norms = np.linalg.norm(X, axis=1)**2\n",
    "    dot = X@X.T\n",
    "    squared_euclidian_distances = norms[:, None] - 2 * dot + norms[None, :]\n",
    "\n",
    "    K = np.exp(-squared_euclidian_distances/2)\n",
    "    \n",
    "    return K\n",
    "\n",
    "def Compute_RFF_Gaussian_Kernel(X, p):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    W = ... # Sample W, it must be a matrix of size p x d\n",
    "    b = ... # Sample b, it must be a matrix of size p x 1\n",
    "    \n",
    "    psiX = ... # Compute psi(X), it must be a matrix of size n x p\n",
    "    \n",
    "    K = ...\n",
    "    \n",
    "    return K\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now play with the following code to see how well the kernel is approximated depending on the number of random fourier features $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X, t = make_s_curve(noise=0.05, random_state=0)\n",
    "sorting = np.argsort(t)\n",
    "X = X[sorting]\n",
    "\n",
    "p_list = [1,10,100,1000] # Add the values of p you want to test \n",
    "\n",
    "p_list = [None] + p_list\n",
    "\n",
    "fig, axes = plt.subplots(1, len(p_list), figsize=(len(p_list)*5, 5))\n",
    "\n",
    "for p in p_list:\n",
    "    \n",
    "    ax = axes[p_list.index(p)]\n",
    "    \n",
    "    if p is None:\n",
    "        K = Compute_Exact_Gaussian_Kernel(X)\n",
    "        ax.set_title(f'Exact')\n",
    "    else:\n",
    "        K = Compute_RFF_Gaussian_Kernel(X, p)\n",
    "        ax.set_title(f'p={p}')\n",
    "        \n",
    "    ax.imshow(K, vmin=0, vmax=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some code to quantitevely compute the approximation error of the Gram matrix as a function of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the formulation of Ridge Regression :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n (\\langle w, x_i \\rangle  - y_i)^2 + \\lambda ||w||_2^2\n",
    "\\end{equation*}\n",
    "\n",
    "whose optimal weight is $$w^* = (X^T X + \\lambda I_d)^{-1}X^T y $$\n",
    "\n",
    "And the kernelized variant:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{f \\in \\mathcal{H}} \\frac{1}{n}\\sum_{i=1}^n ( \\langle f, \\phi(x_i) \\rangle  - y_i)^2 + \\lambda ||f||_\\mathcal{H}^2\n",
    "\\end{equation*}\n",
    "\n",
    "In the previous practical we applied the representer theorem to exactly solve this problem.  Instead, we now propose to use the Random Fourier Feature approximation build in the previous part :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_{w \\in \\mathbb{R}^p} \\frac{1}{n}\\sum_{i=1}^n ( \\langle w, \\psi(x_i) \\rangle  - y_i)^2 + \\lambda ||f||_\\mathcal{H}^2\n",
    "\\end{equation*}\n",
    "\n",
    "This is equivalent to a standard Ridge Regression! Thus the optimal weight is\n",
    "\n",
    "$$ w^* = (\\psi(X)^T \\psi(X) + \\lambda I_p)^{-1} \\psi(X)^T y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) Lets code this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RFF_KRR():\n",
    "    \n",
    "    def __init__(self, p=1, lbda=0.1):\n",
    "        self.p = p\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Sample W and b\n",
    "        n, d = X.shape\n",
    "\n",
    "        self.W = ... # Sample W, it must be a matrix of size p x d\n",
    "        self.b = ... # Sample b, it must be a matrix of size p x 1\n",
    "\n",
    "        # Send to feature space\n",
    "        psiX = ...\n",
    "        \n",
    "        # Center the data\n",
    "        self.mean_x = psiX.mean(axis=0)\n",
    "        psiX = psiX - self.mean_x\n",
    "        \n",
    "        self.mean_y = y.mean()\n",
    "        y = y - self.mean_y\n",
    "        \n",
    "        self.w = ...  # Use np.linalg.solve or np.linalg.lstsq (more stable)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Send to feature space\n",
    "        psiX = ...\n",
    "        \n",
    "        # Center the data\n",
    "        psiX = psiX - self.mean_x\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = psiX@self.w + self.mean_y\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now play with following code to test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.sin(x)\n",
    "X_train = np.linspace(0, 4*np.pi, 100).reshape(-1,1)\n",
    "y_train = f(X_train) + np.random.normal(0,0.5,X_train.shape)\n",
    "X_test = np.linspace(0, 4*np.pi, 1000).reshape(-1,1)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Data', alpha = 0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Train Data Set')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "lbda = 0. # No regularization \n",
    "p_list = [1,...] # Add the values of p you want to test\n",
    "\n",
    "for p in p_list:\n",
    "    \n",
    "    model = RFF_KRR(p=p, lbda=lbda)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.scatter(X_train, y_train, color='blue', label='Data', alpha = 0.5)\n",
    "    plt.plot(X_test, y_pred, color='red', label='Prediction')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'p={p}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider a larger dataset to see if we can observe double descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784) (2434, 784)\n",
      "(12000,) (2434,)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "# Keep only classes 3 and 7\n",
    "X = X[(y == '3') | (y == '7')]\n",
    "y = y[(y == '3') | (y == '7')]\n",
    "y = (y == '7').astype(int)\n",
    "\n",
    "# Normalize the data\n",
    "X = X / 255.0\n",
    "X = X - X.mean(axis=0)\n",
    "\n",
    "# Split the data into train and test\n",
    "n_train = 12000\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rembember that KRR with Random Fourier Features as $p$ parameters where $p$ is the number of random projection. \n",
    "\n",
    "For instance lets fix $p = 100$ so that our model can fit fast.\n",
    "\n",
    "The train set that we just downloaded is of size $n_{\\text{max}} = 12 000$, let's try to truncate it artifically to a smaller size $n$ so that we can compute the performances for differents values of the  relative complexity $C = p/n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def troncate_train_set(X_train, y_train, p, C):\n",
    "    '''\n",
    "    Truncate the train set to n samples\n",
    "    '''\n",
    "    \n",
    "    n = int(p / C)\n",
    "    \n",
    "    assert n <= X_train.shape[0], f'{p} parameters and complexity {C} requires a train set of size {n} which is larger than the max train set of size {X_train.shape[0]}'\n",
    "    \n",
    "    X_train_truncated = X_train[:n]\n",
    "    y_train_truncated = y_train[:n]\n",
    "    \n",
    "    return X_train_truncated, y_train_truncated\n",
    "\n",
    "def Compute_MSE(p, lbda, C):\n",
    "    \n",
    "    # Truncate Data\n",
    "    X_train_truncated, y_train_truncated = ...\n",
    "    \n",
    "    # Define Model\n",
    "    model = ...\n",
    "    \n",
    "    # Fit Model\n",
    "    model.fit(X_train_truncated, y_train_truncated)\n",
    "    \n",
    "    # Get the predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute the Mean Squared Error\n",
    "    mse = ...\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now play with the following code to explore in which regimes does double descent occurs. For the default parameters you should get a nice double descent. \n",
    "\n",
    "Try to increase $\\lambda$ until the double descent disappear. Why does it disappear ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 100 # Higher values might cause problems due to the naive implementation of the RFF_KRR class\n",
    "lbda = 0. # Try with no regularization at first, you can try other values later\n",
    "Cmin = 0.2\n",
    "Cmax = 5\n",
    "\n",
    "C_list = np.linspace(Cmin, Cmax, 50)\n",
    "scores = np.array([Compute_MSE(p = p, lbda = lbda, C = C) for C in C_list])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(C_list, scores, alpha=0.5)\n",
    "plt.plot(C_list, scores, alpha=0.5)\n",
    "plt.xlabel('Relative Complexity C = p/n', fontsize=14)\n",
    "plt.ylabel('MSE', fontsize=14)\n",
    "plt.ylim(0,min(np.max(scores), 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its hard to get overfitting, underfitting and double descent on the same plot. Had a new plot (diffent values of $C$) showing the classic underfitting/overfitting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
